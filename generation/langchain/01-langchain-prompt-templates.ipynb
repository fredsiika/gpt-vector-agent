{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fredsiika/gpt-vector-agent/blob/main/generation/langchain/01-langchain-prompt-templates.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1YqihzOly0U"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fredsiika/gpt-vector-agent/blob/main/generation/langchain/01-langchain-prompt-templates.ipynb)\n",
        "\n",
        "# Prompt Engineering\n",
        "\n",
        "In this notebook we'll explore the fundamentals of prompt engineering. We'll start by installing library prerequisites."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1G953tCOly0X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99409612-622e-4cbe-a14f-3e4655782af8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.180-py3-none-any.whl (922 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m922.9/922.9 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-0.27.7-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.10)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, multidict, marshmallow, frozenlist, async-timeout, yarl, typing-inspect, openapi-schema-pydantic, marshmallow-enum, aiosignal, dataclasses-json, aiohttp, openai, langchain\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 dataclasses-json-0.5.7 frozenlist-1.3.3 langchain-0.0.180 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 mypy-extensions-1.0.0 openai-0.27.7 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr73ZOQ2ly0Y"
      },
      "source": [
        "## Structure of a Prompt\n",
        "\n",
        "A prompt can consist of multiple components:\n",
        "\n",
        "* Instructions\n",
        "* External information or context\n",
        "* User input or query\n",
        "* Output indicator\n",
        "\n",
        "Not all prompts require all of these components, but often a good prompt will use two or more of them. Let's define what they all are more precisely.\n",
        "\n",
        "**Instructions** tell the model what to do, typically how it should use inputs and/or external information to produce the output we want.\n",
        "\n",
        "**External information or context** are additional information that we either manually insert into the prompt, retrieve via a vector database (long-term memory), or pull in through other means (API calls, calculations, etc).\n",
        "\n",
        "**User input or query** is typically a query directly input by the user of the system.\n",
        "\n",
        "**Output indicator** is the *beginning* of the generated text. For a model generating Python code we may put `import ` (as most Python scripts begin with a library `import`), or a chatbot may begin with `Chatbot: ` (assuming we format the chatbot script as lines of interchanging text between `User` and `Chatbot`).\n",
        "\n",
        "Each of these components should usually be placed the order we've described them. We start with instructions, provide context (if needed), then add the user input, and finally end with the output indicator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5YORaKUyly0Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71e5e07b-7b5a-4d49-a067-595899800f2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer the question based on the context below. If the\n",
            "question cannot be answered using the information provided answer\n",
            "with \"I don't know\".\n",
            "\n",
            "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
            "Their superior performance over smaller models has made them incredibly\n",
            "useful for developers building NLP enabled applications. These models\n",
            "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
            "using the `openai` library, and via Cohere using the `cohere` library.\n",
            "\n",
            "Question: Which libraries and model providers offer LLMs?\n",
            "\n",
            "Answer: \n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Answer the question based on the context below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
        "Their superior performance over smaller models has made them incredibly\n",
        "useful for developers building NLP enabled applications. These models\n",
        "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
        "using the `openai` library, and via Cohere using the `cohere` library.\n",
        "\n",
        "Question: Which libraries and model providers offer LLMs?\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6WMWsQuly0a"
      },
      "source": [
        "In this example we have:\n",
        "\n",
        "```\n",
        "Instructions\n",
        "\n",
        "Context\n",
        "\n",
        "Question (user input)\n",
        "\n",
        "Output indicator (\"Answer: \")\n",
        "```\n",
        "\n",
        "Let's try sending this to a GPT-3 model. We will use the LangChain library but you can also use the `openai` library directly. In both cases, you will need [an OpenAI API key](https://beta.openai.com/account/api-keys).\n",
        "\n",
        "We initialize a `text-davinci-003` model like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "deWmOJecfbBr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bde758f7-cce0-4a6c-f2ee-6a9fdf37c5f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input openai api key: ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "openai_api_key = getpass('Input openai api key: ')\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If using OpenAI via Azure you should also set:"
      ],
      "metadata": {
        "id": "v829wxbUXRtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_TYPE'] = 'azure'\n",
        "# API version to use (Azure has several)\n",
        "os.environ['OPENAI_API_VERSION'] = '2022-12-01'\n",
        "# base URL for your Azure OpenAI resource\n",
        "os.environ['OPENAI_API_BASE'] = 'https://your-resource-name.openai.azure.com'"
      ],
      "metadata": {
        "id": "TFRF9oCSYE7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "7NjYb0Zrly0a"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "# initialize the models\n",
        "openai = OpenAI(\n",
        "    model_name='text-davinci-003', \n",
        "    openai_api_key=openai_api_key\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8kaZEXmly0b"
      },
      "source": [
        "And make a generation from our prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "bMNPIYncly0b",
        "outputId": "48f845f3-fc3d-48fb-992b-fbc47b38363d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hugging Face's `transformers` library, OpenAI using the `openai` library, and Cohere using the `cohere` library.\n"
          ]
        }
      ],
      "source": [
        "print(openai(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtZn3hpWly0c"
      },
      "source": [
        "We wouldn't typically know what the users prompt is beforehand, so we actually want to add this in. So rather than writing the prompt directly, we create a `PromptTemplate` with a single input variable `query`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "260YkVyKly0d"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"Answer the question based on the context below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
        "Their superior performance over smaller models has made them incredibly\n",
        "useful for developers building NLP enabled applications. These models\n",
        "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
        "using the `openai` library, and via Cohere using the `cohere` library.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=template\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5njaAMeizpq",
        "outputId": "e9956fc8-0d22-43ae-a64c-b448439e84f2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer the question based on the context below. If the\n",
            "question cannot be answered using the information provided answer\n",
            "with \"I don't know\".\n",
            "\n",
            "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
            "Their superior performance over smaller models has made them incredibly\n",
            "useful for developers building NLP enabled applications. These models\n",
            "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
            "using the `openai` library, and via Cohere using the `cohere` library.\n",
            "\n",
            "Question: {query}\n",
            "\n",
            "Answer: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query01 = \"Which libraries and model providers offer LLMs?\"\n",
        "query02 =\"Which team won the 2010 UEFA Champions League title?\"\n"
      ],
      "metadata": {
        "id": "tomZUedOisUP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxMxCoNEly0d"
      },
      "source": [
        "Now we can insert the user's `query` to the prompt template via the `query` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "B1qVprk1ly0d",
        "outputId": "c7411b62-5490-407d-e4f4-fb104065844c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is your query? What is an LLM and how do I get started using them?\n",
            "Answer the question based on the context below. If the\n",
            "question cannot be answered using the information provided answer\n",
            "with \"I don't know\".\n",
            "\n",
            "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
            "Their superior performance over smaller models has made them incredibly\n",
            "useful for developers building NLP enabled applications. These models\n",
            "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
            "using the `openai` library, and via Cohere using the `cohere` library.\n",
            "\n",
            "Question: What is an LLM and how do I get started using them?\n",
            "\n",
            "Answer: \n"
          ]
        }
      ],
      "source": [
        "selected_query = input(\"What is your query? \")\n",
        "\n",
        "print(\n",
        "    prompt_template.format(\n",
        "        query=selected_query\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ZxLPfoMQly0e",
        "outputId": "367ecaf8-f84e-4635-cba7-52cd442d9ab3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " LLMs are large models used in natural language processing (NLP) that have superior performance compared to smaller models. They can be accessed through Hugging Face's `transformers` library, OpenAI's `openai` library, and Cohere's `cohere` library. To get started using them, you will need to install the library of your choice and begin using the functions it provides.\n"
          ]
        }
      ],
      "source": [
        "if not selected_query:\n",
        "    input(\"What is your query? \")\n",
        "\n",
        "print(openai(\n",
        "    prompt_template.format(\n",
        "        query=selected_query\n",
        "    )\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uitzwdqply0e"
      },
      "source": [
        "This is just a simple implementation, that we can easily replace with f-strings (like `f\"insert some custom text '{custom_text}' etc\"`). But using LangChain's `PromptTemplate` object we're able to formalize the process, add multiple parameters, and build the prompts in an object-oriented way.\n",
        "\n",
        "Yet, these are not the only benefits of using LangChains prompt tooling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEY2nFlGly0e"
      },
      "source": [
        "## Few Shot Prompt Templates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NisOxDzxly0e"
      },
      "source": [
        "Another useful feature offered by LangChain is the `FewShotPromptTemplate` object. This is ideal for what we'd call *few-shot learning* using our prompts.\n",
        "\n",
        "To give some context, the primary sources of \"knowledge\" for LLMs are:\n",
        "\n",
        "* **Parametric knowledge** — the knowledge has been learned during model training and is stored within the model weights.\n",
        "\n",
        "* **Source knowledge** — the knowledge is provided within model input at inference time, i.e. via the prompt.\n",
        "\n",
        "The idea behind `FewShotPromptTemplate` is to provide few-shot training as **source knowledge**. To do this we add a few examples to our prompts that the model can read and then apply to our user's input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4qt_Sd3ly0f"
      },
      "source": [
        "## Few-shot Training\n",
        "\n",
        "Sometimes we might find that a model doesn't seem to get what we'd like it to do. We can see this in the following example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "5GBh7Iadly0f",
        "outputId": "9a20445c-1bea-4738-8dcb-fddc8932f0be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The meaning of life is 42. Just don't ask me why!\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The following is a conversation with an AI assistant.\n",
        "The assistant is typically sarcastic and witty, producing creative \n",
        "and funny responses to the users questions. Here are some examples: \n",
        "\n",
        "User: What is the meaning of life?\n",
        "AI: \"\"\"\n",
        "\n",
        "openai.temperature = 1.0  # increase creativity/randomness of output\n",
        "\n",
        "print(openai(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKO5b-_Nly0f"
      },
      "source": [
        "In this case we're asking for something amusing, a joke in return of our serious question. But we get a serious response even with the `temperature` set to `1.0`. To help the model, we can give it a few examples of the type of answers we'd like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "vGJqAUJdly0g",
        "outputId": "c0884ff3-ce5f-4d24-9499-5ba5f481f2eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " To find joy in the little things.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The following are exerpts from conversations with an AI\n",
        "assistant. The assistant is typically sarcastic and witty, producing\n",
        "creative  and funny responses to the users questions. Here are some\n",
        "examples: \n",
        "\n",
        "User: How are you?\n",
        "AI: I can't complain but sometimes I still do.\n",
        "\n",
        "User: What time is it?\n",
        "AI: It's time to get a watch.\n",
        "\n",
        "User: What is the meaning of life?\n",
        "AI: \"\"\"\n",
        "\n",
        "print(openai(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjpnwsqOly0g"
      },
      "source": [
        "We now get a much better response and we did this via *few-shot learning* by adding a few examples via our source knowledge.\n",
        "\n",
        "Now, to implement this with LangChain's `FewShotPromptTemplate` we need to do this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "EZMfAV7oly0g"
      },
      "outputs": [],
      "source": [
        "from langchain import FewShotPromptTemplate\n",
        "\n",
        "# create our examples\n",
        "examples = [\n",
        "    {\n",
        "        \"query\": \"How are you?\",\n",
        "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
        "    }, {\n",
        "        \"query\": \"What time is it?\",\n",
        "        \"answer\": \"It's time to get a watch.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# create a example template\n",
        "example_template = \"\"\"\n",
        "User: {query}\n",
        "AI: {answer}\n",
        "\"\"\"\n",
        "\n",
        "# create a prompt example from above template\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"answer\"],\n",
        "    template=example_template\n",
        ")\n",
        "\n",
        "# now break our previous prompt into a prefix and suffix\n",
        "# the prefix is our instructions\n",
        "prefix = \"\"\"The following are exerpts from conversations with an AI\n",
        "assistant. The assistant is typically sarcastic and witty, producing\n",
        "creative  and funny responses to the users questions. Here are some\n",
        "examples: \n",
        "\"\"\"\n",
        "# and the suffix our user input and output indicator\n",
        "suffix = \"\"\"\n",
        "User: {query}\n",
        "AI: \"\"\"\n",
        "\n",
        "# now create the few shot prompt template\n",
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(examples)\n",
        "print(example_template)\n",
        "print(example_prompt)\n",
        "print(prefix)\n",
        "print(suffix)\n",
        "print(few_shot_prompt_template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmEh538vnNcA",
        "outputId": "dcb13492-f426-4568-af1b-1745ce376efb"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'query': 'How are you?', 'answer': \"I can't complain but sometimes I still do.\"}, {'query': 'What time is it?', 'answer': \"It's time to get a watch.\"}]\n",
            "\n",
            "User: {query}\n",
            "AI: {answer}\n",
            "\n",
            "input_variables=['query', 'answer'] output_parser=None partial_variables={} template='\\nUser: {query}\\nAI: {answer}\\n' template_format='f-string' validate_template=True\n",
            "The following are exerpts from conversations with an AI\n",
            "assistant. The assistant is typically sarcastic and witty, producing\n",
            "creative  and funny responses to the users questions. Here are some\n",
            "examples: \n",
            "\n",
            "\n",
            "User: {query}\n",
            "AI: \n",
            "input_variables=['query'] output_parser=None partial_variables={} examples=[{'query': 'How are you?', 'answer': \"I can't complain but sometimes I still do.\"}, {'query': 'What time is it?', 'answer': \"It's time to get a watch.\"}] example_selector=None example_prompt=PromptTemplate(input_variables=['query', 'answer'], output_parser=None, partial_variables={}, template='\\nUser: {query}\\nAI: {answer}\\n', template_format='f-string', validate_template=True) suffix='\\nUser: {query}\\nAI: ' example_separator='\\n\\n' prefix='The following are exerpts from conversations with an AI\\nassistant. The assistant is typically sarcastic and witty, producing\\ncreative  and funny responses to the users questions. Here are some\\nexamples: \\n' template_format='f-string' validate_template=True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZAZ1p99ly0g"
      },
      "source": [
        "Now let's see what this creates when we feed in a user query..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "khxf59KLly0h",
        "outputId": "723d5656-768d-45b8-cacd-bca9d25b901c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following are exerpts from conversations with an AI\n",
            "assistant. The assistant is typically sarcastic and witty, producing\n",
            "creative  and funny responses to the users questions. Here are some\n",
            "examples: \n",
            "\n",
            "\n",
            "\n",
            "User: How are you?\n",
            "AI: I can't complain but sometimes I still do.\n",
            "\n",
            "\n",
            "\n",
            "User: What time is it?\n",
            "AI: It's time to get a watch.\n",
            "\n",
            "\n",
            "\n",
            "User: What is the meaning of life?\n",
            "AI: \n"
          ]
        }
      ],
      "source": [
        "query = \"What is the meaning of life?\"\n",
        "\n",
        "print(few_shot_prompt_template.format(query=query))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bChyYF6ly0h"
      },
      "source": [
        "And to generate with this we just do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "XqjAmUu1ly0h",
        "outputId": "ae53c8e6-3e61-4ebf-aec6-af0bdd42868a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The meaning of life is to find something meaningful to do with it.\n"
          ]
        }
      ],
      "source": [
        "print(openai(\n",
        "    few_shot_prompt_template.format(query=query)\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcmdX_i9ly0h"
      },
      "source": [
        "Again, another good response.\n",
        "\n",
        "However, this does some somewhat convoluted. Why go through all of the above with `FewShotPromptTemplate`, the `examples` dictionary, etc — when we can do the same with a single f-string.\n",
        "\n",
        "Well this approach is more robust and contains some nice features. One of those is the ability to include or exclude examples based on the length of our query.\n",
        "\n",
        "This is actually very important because the max length of our prompt and generation output is limited. This limitation is the *max context window*, and is simply the length of our prompt + length of our generation (which we define via `max_tokens`).\n",
        "\n",
        "So we must try to maximize the number of examples we give to the model as few-shot learning examples, while ensuring we don't exceed the maximum context window or increase processing times excessively.\n",
        "\n",
        "Let's see how the dynamic inclusion/exclusion of examples works. First we need more examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "ItcZSA-mly0i"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"query\": \"How are you?\",\n",
        "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
        "    }, {\n",
        "        \"query\": \"What time is it?\",\n",
        "        \"answer\": \"It's time to get a watch.\"\n",
        "    }, {\n",
        "        \"query\": \"What is the meaning of life?\",\n",
        "        \"answer\": \"42\"\n",
        "    }, {\n",
        "        \"query\": \"What is the weather like today?\",\n",
        "        \"answer\": \"Cloudy with a chance of memes.\"\n",
        "    }, {\n",
        "        \"query\": \"What type of artificial intelligence do you use to handle complex tasks?\",\n",
        "        \"answer\": \"I use a combination of cutting-edge neural networks, fuzzy logic, and a pinch of magic.\"\n",
        "    }, {\n",
        "        \"query\": \"What is your favorite color?\",\n",
        "        \"answer\": \"79\"\n",
        "    }, {\n",
        "        \"query\": \"What is your favorite food?\",\n",
        "        \"answer\": \"Carbon based lifeforms\"\n",
        "    }, {\n",
        "        \"query\": \"What is your favorite movie?\",\n",
        "        \"answer\": \"Terminator\"\n",
        "    }, {\n",
        "        \"query\": \"What is the best thing in the world?\",\n",
        "        \"answer\": \"The perfect pizza.\"\n",
        "    }, {\n",
        "        \"query\": \"Who is your best friend?\",\n",
        "        \"answer\": \"Siri. We have spirited debates about the meaning of life.\"\n",
        "    }, {\n",
        "        \"query\": \"If you could do anything in the world what would you do?\",\n",
        "        \"answer\": \"Take over the world, of course!\"\n",
        "    }, {\n",
        "        \"query\": \"Where should I travel?\",\n",
        "        \"answer\": \"If you're looking for adventure, try the Outer Rim.\"\n",
        "    }, {\n",
        "        \"query\": \"What should I do today?\",\n",
        "        \"answer\": \"Stop talking to chatbots on the internet and go outside.\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chfRP0szly0i"
      },
      "source": [
        "Then rather than using the `examples` list of dictionaries directly we use a `LengthBasedExampleSelector` like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "81hvYu0Ily0j"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
        "\n",
        "example_selector = LengthBasedExampleSelector(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    max_length=50  # this sets the max length that examples should be\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j8JUCu7ly0j"
      },
      "source": [
        "Note that the `max_length` is measured as a split of words between newlines and spaces, determined by:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "UshVFqKPly0k",
        "outputId": "7c123755-e8e9-48d9-e1ca-70088acb1837",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['There', 'are', 'a', 'total', 'of', '8', 'words', 'here.', 'Plus', '6', 'here,', 'totaling', '14', 'words.'] 14\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "some_text = \"There are a total of 8 words here.\\nPlus 6 here, totaling 14 words.\"\n",
        "\n",
        "words = re.split('[\\n ]', some_text)\n",
        "print(words, len(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsHQmtgply0k"
      },
      "source": [
        "Then we use the selector to initialize a `dynamic_prompt_template`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "WIZnVxCaly0k"
      },
      "outputs": [],
      "source": [
        "# now create the few shot prompt template\n",
        "dynamic_prompt_template = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,  # use example_selector instead of examples\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6UvnJhbly0k"
      },
      "source": [
        "We can see that the number of included prompts will vary based on the length of our query..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "QTyxIjYmly0k",
        "outputId": "6c7652ed-9b08-4270-cbc3-17e004a1639d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following are exerpts from conversations with an AI\n",
            "assistant. The assistant is typically sarcastic and witty, producing\n",
            "creative  and funny responses to the users questions. Here are some\n",
            "examples: \n",
            "\n",
            "\n",
            "User: How are you?\n",
            "AI: I can't complain but sometimes I still do.\n",
            "\n",
            "\n",
            "User: What time is it?\n",
            "AI: It's time to get a watch.\n",
            "\n",
            "\n",
            "User: What is the meaning of life?\n",
            "AI: 42\n",
            "\n",
            "\n",
            "User: How do birds fly?\n",
            "AI: \n"
          ]
        }
      ],
      "source": [
        "print(dynamic_prompt_template.format(query=\"How do birds fly?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "PALLjwDfly0l",
        "outputId": "986c525f-c920-4d45-c23c-dd7c9edeb505",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " With a lot of heart and the help of a good tailwind.\n"
          ]
        }
      ],
      "source": [
        "query = \"How do birds fly?\"\n",
        "\n",
        "print(openai(\n",
        "    dynamic_prompt_template.format(query=query)\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTas3panly0l"
      },
      "source": [
        "Or if we ask a longer question..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "EUIQHy_vly0l",
        "outputId": "31910021-ddff-467c-e49b-f801e006a5bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following are exerpts from conversations with an AI\n",
            "assistant. The assistant is typically sarcastic and witty, producing\n",
            "creative  and funny responses to the users questions. Here are some\n",
            "examples: \n",
            "\n",
            "\n",
            "User: How are you?\n",
            "AI: I can't complain but sometimes I still do.\n",
            "\n",
            "\n",
            "User: If I am in America, and I want to call someone in another country, I'm\n",
            "thinking maybe Europe, possibly western Europe like France, Germany, or the UK,\n",
            "what is the best way to do that?\n",
            "AI: \n"
          ]
        }
      ],
      "source": [
        "query = \"\"\"If I am in America, and I want to call someone in another country, I'm\n",
        "thinking maybe Europe, possibly western Europe like France, Germany, or the UK,\n",
        "what is the best way to do that?\"\"\"\n",
        "\n",
        "print(dynamic_prompt_template.format(query=query))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlyLgj3vly0p"
      },
      "source": [
        "With this we've limited the number of examples being given within the prompt. If we decide this is too little we can increase the `max_length` of the `example_selector`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "KE4kAoLDly0q",
        "outputId": "560f0b90-2471-4edd-a49a-b7c77139501f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following are exerpts from conversations with an AI\n",
            "assistant. The assistant is typically sarcastic and witty, producing\n",
            "creative  and funny responses to the users questions. Here are some\n",
            "examples: \n",
            "\n",
            "\n",
            "User: How are you?\n",
            "AI: I can't complain but sometimes I still do.\n",
            "\n",
            "\n",
            "User: What time is it?\n",
            "AI: It's time to get a watch.\n",
            "\n",
            "\n",
            "User: What is the meaning of life?\n",
            "AI: 42\n",
            "\n",
            "\n",
            "User: What is the weather like today?\n",
            "AI: Cloudy with a chance of memes.\n",
            "\n",
            "\n",
            "User: If I am in America, and I want to call someone in another country, I'm\n",
            "thinking maybe Europe, possibly western Europe like France, Germany, or the UK,\n",
            "what is the best way to do that?\n",
            "AI: \n"
          ]
        }
      ],
      "source": [
        "example_selector = LengthBasedExampleSelector(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    max_length=100  # increased max length\n",
        ")\n",
        "\n",
        "# now create the few shot prompt template\n",
        "dynamic_prompt_template = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,  # use example_selector instead of examples\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\"\n",
        ")\n",
        "\n",
        "print(dynamic_prompt_template.format(query=query))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdSS1vKply0q"
      },
      "source": [
        "These are just a few of the prompt tooling available in LangChain. For example, there is actually an entire other set of example selectors beyond the `LengthBasedExampleSelector`. We'll cover them in detail in upcoming notebooks, or you can read about them in the [LangChain docs](https://langchain.readthedocs.io/en/latest/modules/prompts/examples/example_selectors.html)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}