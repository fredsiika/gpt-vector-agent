{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "history_visible": true,
      "authorship_tag": "ABX9TyMCA58k/BLKyCrnpo2M0c4H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fredsiika/gpt-vector-agent/blob/main/getting_started/retrieval-augmentation-langchain-pinecone-openai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solving LLM Hallucination with Knowledge Bases\n",
        "\n",
        "- **Large Language Models (LLMs)** have a data freshness problem. Even some of the most powerful models, like ChatGPT's `gpt-3.5-turbo` and `GPT-4`, have no idea about recent events.\n",
        "\n",
        "- The world, according to LLMs, is frozen in time. They only know the world as it appeared through their training data.\n",
        "\n",
        "That creates problems for any use case that relies on up-to-date information or a particular dataset. For example, you may have internal company documents you’d like to interact with via an LLM.\n",
        "\n",
        "The first challenge is adding those documents to the LLM, we could try training the LLM on these documents, but this is time-consuming and expensive. And what happens when a new document is added? Training for every new document added is beyond inefficient — it is simply impossible.\n",
        "\n",
        "So, how do we handle this problem? We can use **retrieval augmentation**. This technique allows us to retrieve relevant information from an external knowledge base and give that information to our LLM.\n",
        "\n",
        "The external knowledge base is our \"window\" into the world beyond the LLM's training data. This colab workspace is my attempt at learning all about **implementing retrieval augmentation for LLMs** using **LangChain** and the **Pinecone vector database**.\n"
      ],
      "metadata": {
        "id": "Pu_JDmEBQy4t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2SIwwLB-Qvmf"
      },
      "outputs": [],
      "source": [
        "!pip3 install -qU langchain openai tiktoken pinecone-client[grpc] datasets apache_beam mwparserfromhell"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import threading \n"
      ],
      "metadata": {
        "id": "L_lkxJ5ExT-M"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade google-auth\n",
        "!pip3 install protobuf==3.19.6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEiTx_o8w3T4",
        "outputId": "c9207f46-2d66-4ff6-fcde-787ff7ab1134"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting protobuf==3.19.6\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.3\n",
            "    Uninstalling protobuf-3.19.3:\n",
            "      Successfully uninstalled protobuf-3.19.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.19.6\n"
          ]
        }
      ]
    }
  ]
}